#!/usr/bin/env python3
# -*- coding: utf-8 -*-
#%%
%matplotlib inline
import os, subprocess, time, sys

import tensorflow as tf
import tensorflow.keras as keras

from tensorflow.keras import datasets
from tensorflow.keras.initializers import glorot_uniform, Constant
from tensorflow.keras.layers import *
from tensorflow.keras.layers import PReLU, LeakyReLU, Conv2D, MaxPool2D, Lambda, Conv2DTranspose, Flatten
from tensorflow.keras.regularizers import l2
from tensorflow.keras.models import load_model
from tensorflow.keras import callbacks
from tensorflow.keras.models import model_from_json
from tensorflow.keras import backend as K
from tensorflow.keras import Model
from tensorflow.python.client import device_lib
from tensorflow.python.keras.utils import conv_utils

from collections import defaultdict
import itertools
import shutil

from IPython.display import clear_output
from PIL import Image

import numpy as np

import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = [10, 5]
from matplotlib.ticker import MaxNLocator

import pickle

import sklearn as skl
from sklearn import datasets, linear_model
from sklearn.model_selection import cross_val_score



import imageio
import random
import scipy

import math

from ipywidgets import interact, fixed, Layout, Box, interactive, VBox, HBox
import ipywidgets as widgets


from DataHandler import *

#%%
my_devices = tf.config.experimental.list_physical_devices(device_type='CPU')
tf.config.experimental.set_visible_devices(devices= my_devices, device_type='CPU')

#%%
print(device_lib.list_local_devices())

imageRes = 224
hiddenFeatureLayerSize = 4096

#%%
## Define some useful functions
class PlotLossAccuracy(keras.callbacks.Callback):
    
    def __init__( self, sampleImage = None, model = None ):
        self.i = 0
        self.x = []
        self.acc = []
        self.losses = []
        self.val_losses = []
        self.val_acc = []
        self.logs = []
        self.model = model
        self.image = sampleImage
    
    def on_train_begin(self, logs={}):
        self.i = self.i

    def on_epoch_end(self, epoch, logs={}):
        
        self.logs.append(logs)
        self.x.append(int(self.i))
        self.losses.append(logs.get('loss'))
        self.val_losses.append(logs.get('val_loss'))
        self.acc.append(logs.get('acc'))
        self.val_acc.append(logs.get('val_acc'))
        
        self.i += 1
        clear_output(wait=True)
        plt.figure( figsize=(32, 12))
        
        plt.plot([1, 4])
        
        plt.subplot( 141 )
        plt.plot(self.x, self.losses, label="train loss")
        plt.plot(self.x, self.val_losses, label="validation loss")
        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))
        plt.ylabel('loss')
        plt.xlabel('epoch')
        plt.title('Model Loss')
        plt.legend()
        
        plt.subplot( 142 )
        plt.plot(self.x, self.acc, label="training accuracy")
        plt.plot(self.x, self.val_acc, label="validation accuracy")
        plt.legend()
        plt.ylabel('accuracy')
        plt.xlabel('epoch')
        plt.title('Model Accuracy')
        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))
        
        if( self.image is not None ):
            plt.subplot( 143 )
            plt.imshow( self.image[ 0 ] )
            
            plt.subplot( 144 )
            prediction = self.model.predict( self.image )[ 0 ]
            plt.imshow( prediction )
        
        plt.show();
        
def save_model_to_disk():    
    # save model and weights (don't change the filenames)
    model_json = model.to_json()
    with open("model.json", "w") as json_file:
        json_file.write(model_json)
    # serialize weights to HDF5
    model.save_weights("model.h5")
    print("Saved model to model.json and weights to model.h5")
    
def load_model_from_disk():
    with open("model.json", "r") as json_file:
        return model_from_json( json_file.read() )
    
#%%
playlistHandler = PlaylistHandler( "https://www.youtube.com/playlist?list=PLvTaGTS7sTNyxehFKJqggR02dxABc-yBm" )
#%%
from SqueezeNet import *

autoEncoderModel = SqueezeNetAutoencoder()
inputObj = Input( ( 224, 224, 3 ) )
outputObj = autoEncoderModel( inputObj )
modelComp = Model( inputObj, outputObj, name='SqueezenetAutoencoder' )
modelComp.summary()

#%%

from BasicModel import *
isLstm = False
# Create an instance of the model
model = FrameAutoencoderModel()
#inputObj = Input( batch_shape=( None, None, 128, 128, 3 ) )
inputObj = Input( ( 128, 128, 3 ) )
outputObj = model( inputObj )
modelComp = Model( inputObj, outputObj, name='inception_v4' )
modelComp.summary()


#%%

mse = tf.keras.losses.MeanSquaredError()
#opt = keras.optimizers.SGD( lr=0.1, decay=1e-8, momentum=0.9, nesterov=True )
opt = keras.optimizers.Adam( lr=0.0001, decay=1e-8 )
model.compile(optimizer=opt,
              loss=mse,
              metrics=['acc'])

#%%
# pltCallBack = PlotLossAccuracy()
# tbCallback = keras.callbacks.TensorBoard( log_dir='./logs', write_images=True )

# model.fit(x_train, y_train, epochs=5, callbacks=[pltCallBack, tbCallback])

# Train autoencoder
num_epochs = 64

isLstm = False
macroBatchSize = 512
numMacroBatches = playlistHandler.numVideoBatches( macroBatchSize )

sampleImage = playlistHandler.loadVideoBatch( 120, 1 )
# Create an instance of our callback functions class, to plot our loss function and accuracy with each epoch.
pltCallBack = PlotLossAccuracy( sampleImage, model )
tbCallback = keras.callbacks.TensorBoard( log_dir='./logs', write_images=True )
blockSize = 0
batchSize = 64
timeSteps = 2
startAt = 8

for e in range( num_epochs ):
    for mvb in range( numMacroBatches ):
        macroVideoBatch = playlistHandler.loadVideoBatch( mvb, macroBatchSize )
        train_in, validation_in = skl.model_selection.train_test_split( macroVideoBatch, test_size=0.05, random_state=0 )
        train_out = train_in
        validation_out = validation_in
        model.fit( train_in, train_out, 
                   batch_size=batchSize, epochs=1,
                   validation_data=( validation_in, validation_out ),
                   callbacks=[ pltCallBack ] )#, tbCallback ] )


#%%
import math
dataHandler = playlistHandler.dataHandlerList[ 3 ]

chunkSize = 1024
numChunks = dataHandler.getTimeBlockCount( chunkSize, 0 )

for i in range( numChunks ):
    frames = dataHandler.getVideoTimeBatches( i, 0, chunkSize )
    predictions = ( model.predict( frames ) * 255.0 ).astype( np.uint8 )
    frames = ( frames * 255.0 ).astype( np.uint8 )
    
    for idx, image in enumerate( predictions ):
        srcImage = frames[ idx ]
        estImage = image
        imagePath = os.path.join( "./imageOutputs", "{0:05d}.bmp".format( ( i * chunkSize ) + idx ) )
        imageio.imwrite( imagePath, np.concatenate( (srcImage, estImage), 1 ) )
    
command = 'ffmpeg -r 25 -start_number 0 -i imageOutputs/%05d.bmp -i {} -c:v libx264 -vf "scale=512:256,fps=25,format=yuv420p" outputFromAudio3.mp4'.format( dataHandler.audioTrackPath )
#command = "ffmpeg -r 1/5 -i ./output/out%d.png -c:v libx264 -vf scale=256:256 fps=25 -pix_fmt yuv420p out.mp4"
subprocess.run( command, shell=True, capture_output=True )

#%%

modelPath = os.path.join("./ailbheBasicModel_saved_model", "")
modelPath2 = os.path.join("./ailbheBasicModel_model_save", "")
modelPath3 = "./ailbheBasicModel_model_weights"
tf.saved_model.save( model, modelPath )
model.save( modelPath2 )
model.save( modelPath3 )

#%%
#model.save_weights( "./ailbheModelSqueezeNet" )
#%%
autoEncoderModel.load_weights( "ailbheModelSqueezeNet" )
#%%
converter = tf.lite.TFLiteConverter.from_keras_model( autoEncoderModel )
tflite_model = converter.convert()

# Save the TF Lite model.
with tf.io.gfile.GFile( 'autoEncoderModel.tflite', 'wb' ) as f:
  f.write(tflite_model)

#%%
audioFFTLength = 128
hiddenLayerSize = 1024

class AudioFeatureExtractor( Model ):
    def __init__( self, hiddenLayerSize, inputSize, **kwargs ):
        super( AudioFeatureExtractor, self ).__init__( **kwargs )
        #self.inputLayer = Input( [ 1, inputSize ] )
        self.inputLayer = Input( batch_shape=( 1, 1, inputSize ) )
        
        self.lstm1 = LSTM( inputSize, stateful=True )
        self.dropout1 = tf.keras.layers.Dropout( 0.3 )
        self.d1 = Dense( hiddenLayerSize, activation='relu' )
        self.dropout2 = tf.keras.layers.Dropout( 0.3 )
        self.d2 = Dense( hiddenLayerSize, activation='sigmoid' )
        self.outputLayer = self.call( self.inputLayer )
        
        super( AudioFeatureExtractor, self ).__init__( inputs=self.inputLayer, outputs=self.outputLayer, **kwargs )
    
    def build( self ):
        # Initialize the graph
        self._is_graph_network = True
        self._init_graph_network(
            inputs=self.inputLayer,
            outputs=self.outputLayer
        )
        
    def call( self, x, training=False ):
        x = self.lstm1( x )
        x = self.dropout1( x )
        x = self.d1( x )
        x = self.dropout2( x )
        x = self.d2( x )
        return x
    

isLstm = True
# Create an instance of the model
audioModelInst = AudioFeatureExtractor( hiddenLayerSize, audioFFTLength )
audioModelInst.summary()
inputObj = Input( batch_shape=( 1, 1, audioFFTLength ) )
#inputObj = Input( ( None, audioFFTLength ) )
outputObj = audioModelInst( inputObj )
audioModel = Model( inputObj, outputObj, name='AudioModel' )

mse = tf.keras.losses.MeanSquaredError()
#opt = keras.optimizers.SGD( lr=0.1, decay=1e-8, momentum=0.9, nesterov=True )
opt = keras.optimizers.Adam( lr=0.0001, decay=1e-8 )
audioModel.compile( optimizer=opt, loss=mse, metrics=['acc'] )


#%%
videoModel = model
videoModelInputObj = Input( ( imageRes, imageRes, 3 ) )
videoEncoderOutputObj = videoModel.encoder( videoModelInputObj )

videoEncoderFn = keras.backend.function( videoModelInputObj, videoEncoderOutputObj )


#%%
audioTimesteps = 64
extractBlockSize = 128

#%%
playlistHandler.loadFileList()
numFrames = len( playlistHandler.allFileList )
#%%
print( "Getting data for {} frames".format( numFrames ) )

audioData = np.empty( ( numFrames, audioTimesteps, audioFFTLength ), dtype='float32' )
encodedData = np.empty( ( numFrames, hiddenLayerSize ), dtype='float32' )

curIdx = 0
for dataHandler in playlistHandler.dataHandlerList:
    numBlocks = dataHandler.getBlockCount( extractBlockSize )
    for i in range( numBlocks ):
        audioBatches, videoBatches = dataHandler.getAudioVideoData( i, audioTimesteps, videoTimesteps=0, batchSize=extractBlockSize )
        numItemsInBlock = audioBatches.shape[ 0 ]
        lastIdx = curIdx + numItemsInBlock
        audioData[ curIdx : lastIdx ] = audioBatches        
        encodedData[ curIdx : lastIdx ] = videoEncoderFn( videoBatches )
        
        curIdx = lastIdx

indices = np.arange( numFrames )
np.random.shuffle( indices )

audioData = audioData[ indices ]
encodedData = encodedData[ indices ]
#%%
# Train audio encoder
num_epochs = 4096
# Create an instance of our callback functions class, to plot our loss function and accuracy with each epoch.
pltCallBack = PlotLossAccuracy()
#tbCallback = keras.callbacks.TensorBoard( log_dir='./logs', write_images=True )
blockSize = 0
batchSize = 128
timeSteps = 2
startAt = 8
audioTimesteps = 64

#%%
audioModel.fit( audioData, encodedData,
                batch_size=batchSize, epochs=1024,
                validation_data=( audioData, encodedData ),
                callbacks=[ pltCallBack ] )#, tbCallback ] )
#%%
# Train if all data in memory
loopBlockSize = 8192 * 4

batchSize = 512

for e in range( num_epochs ):
    numBlocks = numFrames // loopBlockSize + 1 # It'll probably not be a perfect division
    
    for i in range( numBlocks ):
        startIdx = i * numBlocks
        lastIdx = min( startIdx + numBlocks, numFrames )
        
        audioBlock = audioBatches[ startIdx : lastIdx ]
        encodedBlock = encodedData[ startIdx : lastIdx ]
        audioModel.fit( audioBlock, encodedBlock,
                        batch_size=batchSize, epochs=1,
                        validation_data=( audioBlock, encodedBlock ),
                        callbacks=[ pltCallBack ] )#, tbCallback ] )
#%%
# Train with selective memory loading
loopBlockSize = 1024
audioTimesteps = 64
extractBlockSize = 1024

for e in range( num_epochs ):
    for dataHandler in playlistHandler.dataHandlerList:
        numBlocks = dataHandler.getBlockCount( extractBlockSize )
        for i in range( numBlocks ):
                audioBatches, videoBatches = dataHandler.getAudioVideoData( i, audioTimesteps, videoTimesteps=0, batchSize=extractBlockSize )
                encodedData = videoEncoderFn( videoBatches )
                
                audioModel.fit( audioBatches, encodedData, 
                                batch_size=batchSize, epochs=1,
                                validation_data=( audioBatches, encodedData ),
                                callbacks=[ pltCallBack ] )#, tbCallback ] )
        
    
#%%
#audioModel.save_weights( 'ailbheBasicAudioModel_weights' )
#%%
audioModel.load_weights( 'ailbheBasicAudioModel_weights' )

#%%
audioExtractorStateful = True

class AudioFeatureExtractorLayer( Layer ):
    def __init__( self, hiddenLayerSize, inputSize, isStateful, **kwargs ):
        super( AudioFeatureExtractorLayer, self ).__init__( **kwargs )
        
        self.lstm1 = LSTM( inputSize, stateful=isStateful )
        self.dropout1 = tf.keras.layers.Dropout( 0.3 )
        self.d1 = Dense( hiddenLayerSize, activation='relu' )
        self.dropout2 = tf.keras.layers.Dropout( 0.3 )
        self.d2 = Dense( hiddenLayerSize, activation='sigmoid' )
        
        
    def call( self, x ):
        x = self.lstm1( x )
        x = self.dropout1( x )
        x = self.d1( x )
        x = self.dropout2( x )
        x = self.d2( x )
        return x

class FullModel( Model ):
    def __init__( self, hiddenLayerSize, inputSize, **kwargs ):
        super( FullModel, self ).__init__( **kwargs )
        self.audioLayer = AudioFeatureExtractorLayer( hiddenLayerSize, inputSize, audioExtractorStateful )
        self.videoLayer = SqueezeNetDecoder( hiddenLayerSize )
    
    def call( self, x ):
        x = self.audioLayer( x )
        x = self.videoLayer( x )
        return x

# Create an instance of the model
fullModelInst = FullModel( hiddenLayerSize, audioFFTLength )

if( audioExtractorStateful ):
    inputObj = Input( batch_shape=( 1, 1, audioFFTLength ) )
else:
    inputObj = Input( ( None, audioFFTLength ) )

fullModel = Model( inputObj, fullModelInst( inputObj ), name='FullModel' )

#%%
fullModel.layers[1].layers[0].set_weights( audioModel.get_weights() )
fullModel.layers[1].layers[1].set_weights( autoEncoderModel.layers[ 1 ].get_weights() )
audioModel.save_weights( 'ailbheFullModel_weights' )
#%%
audioModel.load_weights( 'ailbheFullModel_weights' )

#%%
blockSize = 256
dataHandler = playlistHandler.dataHandlerList[ 3 ]
numBlocks = dataHandler.getBlockCount( blockSize )

imgIdx = 0
for i in range( numBlocks ):
    audioBatches, videoBatches = dataHandler.getAudioVideoData( i, audioTimesteps, videoTimesteps=0, batchSize=blockSize )
    predictions = fullModel.predict( audioBatches )
    predictions = ( predictions * 255.0 ).astype( np.uint8 )
    for img in predictions:
        imagePath = os.path.join( "./imageOutputs", "{0:05d}.bmp".format( imgIdx ) )
        imgIdx = imgIdx + 1
        imageio.imwrite( imagePath, img )

#%%
command = 'ffmpeg -r 25 -start_number 0 -i ./imageOutputs/%05d.bmp -i {} -c:v libx264 -vf "scale=512:256,fps=25,format=yuv420p" outputFromAudio4.mp4'.format( dataHandler.audioTrackPath )
#command = "ffmpeg -r 1/5 -i ./output/out%d.png -c:v libx264 -vf scale=256:256 fps=25 -pix_fmt yuv420p out.mp4"
subprocess.run( command, shell=True, capture_output=True )

#%%
audioLoader = AudioLoader( "./data/canon.wav" )
audioCqtData = audioLoader.cqt
numAudioFrames = audioCqtData.shape[ 0 ]
blockSize = 1
audioTimesteps = 1

numBlocks = math.ceil( float( numAudioFrames ) / float( blockSize ) )

imgIdx = 0
for i in range( numBlocks ):
    j = i * blockSize
    audioSlice = audioLoader.getAudioTimeBatches( i=i, timeSteps=audioTimesteps, batchSize=blockSize )
    predictions = ( fullModel.predict( audioSlice ) * 255.0 ).astype( np.uint8 )
    for img in predictions:
        imagePath = os.path.join( "./imageOutputs", "{0:05d}.bmp".format( imgIdx ) )
        imgIdx = imgIdx + 1
        imageio.imwrite( imagePath, img )
#%%
command = 'ffmpeg -r 25 -start_number 0 -i ./imageOutputs/%05d.bmp -i {} -c:v libx264 -vf "scale=512:256,fps=25,format=yuv420p" outputFromAilbheAudio2.mp4'.format( './data/canon.wav' )
#command = "ffmpeg -r 1/5 -i ./output/out%d.png -c:v libx264 -vf scale=256:256 fps=25 -pix_fmt yuv420p out.mp4"
subprocess.run( command, shell=True, capture_output=True )

